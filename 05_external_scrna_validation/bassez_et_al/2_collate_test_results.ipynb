{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58b7cc55-3b46-46c2-bdd6-6e34ffdc37f1",
   "metadata": {},
   "source": [
    "# Collate model predictions from purity-level partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a63a1-8e52-493c-b685-5a419fa97191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as adata\n",
    "import scanpy as sc\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly as plotly\n",
    "import plotly.express as px\n",
    "\n",
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730a483-c2d7-42fa-af67-8452637f6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"???/deconvolution_benchmarking/05_external_scrna_validation/bassez_et_al\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e9580-601c-4e92-bc24-6ddecf542c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training patient IDs\n",
    "train_p_ids = [\n",
    "    # HER2+\n",
    "    \"BIOKEY_13\",\n",
    "    # ER+\n",
    "    \"BIOKEY_3\",\n",
    "    \"BIOKEY_5\",\n",
    "    \"BIOKEY_12\",\n",
    "    \"BIOKEY_18\",\n",
    "    \"BIOKEY_22\",\n",
    "    \"BIOKEY_24\",\n",
    "    \"BIOKEY_27\",\n",
    "    \"BIOKEY_29\",\n",
    "    \"BIOKEY_30\",\n",
    "    # \"BIOKEY_32\",\n",
    "    \"BIOKEY_40\",\n",
    "    \"BIOKEY_42\",\n",
    "    # TNBC\n",
    "    \"BIOKEY_2\",\n",
    "    \"BIOKEY_9\",\n",
    "    \"BIOKEY_10\",\n",
    "    \"BIOKEY_11\",\n",
    "    \"BIOKEY_14\",\n",
    "    \"BIOKEY_15\",\n",
    "    \"BIOKEY_33\",\n",
    "    \"BIOKEY_35\",\n",
    "    \"BIOKEY_36\",\n",
    "    \"BIOKEY_41\",\n",
    "]\n",
    "# Test patient IDs\n",
    "test_p_ids = [\n",
    "    # HER2+\n",
    "    \"BIOKEY_28\",\n",
    "    # ER+\n",
    "    \"BIOKEY_4\",\n",
    "    \"BIOKEY_6\",\n",
    "    \"BIOKEY_7\",\n",
    "    \"BIOKEY_17\",\n",
    "    \"BIOKEY_21\",\n",
    "    \"BIOKEY_37\",\n",
    "    # TNBC\n",
    "    \"BIOKEY_1\",\n",
    "    \"BIOKEY_16\",\n",
    "    \"BIOKEY_19\",\n",
    "    \"BIOKEY_26\",\n",
    "    \"BIOKEY_31\",\n",
    "]\n",
    "purity_levels = np.arange(0.05, 1, 0.05).round(3).tolist()\n",
    "c_types = [\n",
    "    \"Cancer_cell\",\n",
    "    \"T_cell\",\n",
    "    \"B_cell\",\n",
    "    \"Myeloid_cell\",\n",
    "    \"Endothelial_cell\",\n",
    "    \"Fibroblast\",\n",
    "    \"Mast_cell\",\n",
    "    \"pDC\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0976ba6",
   "metadata": {},
   "source": [
    "## Prepare our groundtruth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf42dd16",
   "metadata": {},
   "source": [
    "If we haven't extracted groundtruth from test AnnData object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785d40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adata = sc.read_h5ad(Path(prefix).joinpath(\"data/test/test_sim_mixts.h5ad\"))\n",
    "truth_df = test_adata.obs.drop([\"batch\"], axis=1).fillna(0)\n",
    "truth_df = truth_df[c_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into csv beautifully\n",
    "truth_df.to_csv(Path(prefix).joinpath(\"data/results/truth.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91fcfd27",
   "metadata": {},
   "source": [
    "If we have already extracted the groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f562086a-5077-43a2-989e-422ee88109af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load truth.tsv\n",
    "truth_df = pd.read_csv(\n",
    "    Path(prefix).joinpath(\"data/results/truth.tsv\"), sep=\"\\t\", index_col=0\n",
    ")\n",
    "truth_df.columns = c_types\n",
    "truth_df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6fcc9c0-de70-498e-9bf9-d677e877a119",
   "metadata": {},
   "source": [
    "### CIBERSORTx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e2eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we run in normal mode, the results file is called CIBERSORTx_Results\n",
    "# If we run in Smode or Bmode, the results file will be called CIBERSORTx_Adjusted.txt\n",
    "# Adjust the filename accordingy\n",
    "results_f = \"CIBERSORTx_Results.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ec1b6-6624-4277-9bb5-7f50f2d348be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty list to store average of (preds - truth) of each purity levels\n",
    "avg_diff_l = []\n",
    "preds_l = []\n",
    "\n",
    "# Iterate over purity levels\n",
    "for pur_lvl in tqdm(purity_levels):\n",
    "    # Read and reorganize  index and columns to match truth_df\n",
    "    subset_preds_df = pd.read_csv(\n",
    "        Path(prefix).joinpath(f\"data/cbx/results/{pur_lvl}/{results_f}\"),\n",
    "        sep=\"\\t\",\n",
    "        index_col=0,\n",
    "    )\n",
    "    subset_truth_df = truth_df[truth_df[\"Cancer_cell\"] == pur_lvl]\n",
    "    subset_preds_df.drop([\"P-value\", \"Correlation\", \"RMSE\"], axis=1, inplace=True)\n",
    "    preds_l.append(subset_preds_df)\n",
    "\n",
    "    # Calcuate preds-truth for each purity level\n",
    "    diff_df = abs(subset_preds_df[c_types].sort_index() - subset_truth_df.sort_index())\n",
    "    # diff_df = subset_preds_df[c_types] - subset_truth_df\n",
    "    avg_diff = diff_df.mean().to_frame()\n",
    "    avg_diff.columns = [pur_lvl]\n",
    "\n",
    "    avg_diff_l.append(avg_diff)\n",
    "\n",
    "avg_diff_df = pd.concat(avg_diff_l, axis=1)\n",
    "preds_df = pd.concat(preds_l, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae072859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "preds_df.to_csv(Path(prefix).joinpath(\"data/results/cbx.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41e4b1f2-ef03-4f8f-a5de-3da2884fc6cd",
   "metadata": {},
   "source": [
    "### Scaden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223be65-2e97-4cb4-92ea-bf5621cc7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_diff_l = []\n",
    "preds_l = []\n",
    "\n",
    "for pur_lvl in tqdm(purity_levels):\n",
    "    subset_preds_df = pd.read_csv(\n",
    "        Path(prefix).joinpath(f\"data/scaden/{pur_lvl}/results_{pur_lvl}.tsv\"),\n",
    "        sep=\"\\t\",\n",
    "        index_col=0,\n",
    "    )\n",
    "    subset_truth_df = truth_df[truth_df[\"Cancer_cell\"] == pur_lvl]\n",
    "\n",
    "    # Calcuate preds-truth for each purity level\n",
    "    diff_df = abs(subset_preds_df[c_types].sort_index() - subset_truth_df.sort_index())\n",
    "    avg_diff = diff_df.mean().to_frame()\n",
    "    avg_diff.columns = [pur_lvl]\n",
    "\n",
    "    preds_l.append(subset_preds_df)\n",
    "    avg_diff_l.append(avg_diff)\n",
    "\n",
    "preds_df = pd.concat(preds_l, axis=0)\n",
    "avg_diff_df = pd.concat(avg_diff_l, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e25770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "preds_df.to_csv(Path(prefix).joinpath(\"data/results/scaden.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb013ab5-c4d4-4556-baaf-93229c915b70",
   "metadata": {},
   "source": [
    "### EPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d52d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty list to store average of (preds - truth) of each purity levels\n",
    "avg_diff_l = []\n",
    "preds_l = []\n",
    "\n",
    "# Iterate over purity levels\n",
    "for pur_lvl in tqdm(purity_levels):\n",
    "    # Read predictions\n",
    "    subset_preds_df = pd.read_csv(\n",
    "        Path(prefix).joinpath(\n",
    "            f\"data/epic/cbx_sig_matrix/results/{pur_lvl}/results.csv\"\n",
    "        ),\n",
    "        sep=\",\",\n",
    "        index_col=0,\n",
    "    )\n",
    "\n",
    "    # Replace otherCells in predictions by Cancer Epithelial\n",
    "    subset_preds_df.rename(\n",
    "        columns={\n",
    "            \"otherCells\": \"Cancer_cell\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    subset_truth_df = truth_df[truth_df[\"Cancer_cell\"] == pur_lvl]\n",
    "\n",
    "    # Calcuate preds-truth for each purity level\n",
    "    diff_df = abs(subset_preds_df[c_types].sort_index() - subset_truth_df.sort_index())\n",
    "    # diff_df = subset_preds_df[c_types] - subset_truth_df\n",
    "    avg_diff = diff_df.mean().to_frame()\n",
    "    avg_diff.columns = [pur_lvl]\n",
    "\n",
    "    preds_l.append(subset_preds_df)\n",
    "    avg_diff_l.append(avg_diff)\n",
    "\n",
    "avg_diff_df = pd.concat(avg_diff_l, axis=1)\n",
    "preds_df = pd.concat(preds_l, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32afdda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "preds_df.to_csv(Path(prefix).joinpath(\"data/results/epic.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64a9ce78",
   "metadata": {},
   "source": [
    "### bisque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14da92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty list to store average of (preds - truth) of each purity levels\n",
    "experiments = [\"scaled\"]\n",
    "\n",
    "for expt in experiments:\n",
    "\n",
    "    avg_diff_l = []\n",
    "    preds_l = []\n",
    "\n",
    "    # Iterate over purity levels\n",
    "    for pur_lvl in tqdm(purity_levels):\n",
    "        # Read predictions\n",
    "        subset_preds_df = pd.read_csv(\n",
    "            Path(prefix).joinpath(f\"data/bisque/results_{expt}/{pur_lvl}/results.csv\"),\n",
    "            sep=\",\",\n",
    "            index_col=0,\n",
    "        ).T\n",
    "\n",
    "        # Get correct groundtruth subset\n",
    "        subset_truth_df = truth_df[truth_df[\"Cancer_cell\"] == pur_lvl]\n",
    "\n",
    "        # Calcuate preds-truth for each purity level\n",
    "        diff_df = abs(\n",
    "            subset_preds_df[c_types].sort_index() - subset_truth_df.sort_index()\n",
    "        )\n",
    "        # diff_df = subset_preds_df[c_types] - subset_truth_df\n",
    "        avg_diff = diff_df.mean().to_frame()\n",
    "        avg_diff.columns = [pur_lvl]\n",
    "\n",
    "        preds_l.append(subset_preds_df)\n",
    "        avg_diff_l.append(avg_diff)\n",
    "\n",
    "    preds_df = pd.concat(preds_l, axis=0)\n",
    "    avg_diff_df = pd.concat(avg_diff_l, axis=1)\n",
    "\n",
    "    # Save predictions\n",
    "    preds_df.to_csv(Path(prefix).joinpath(f\"data/results/bisque_{expt}.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a60dd49",
   "metadata": {},
   "source": [
    "### DWLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253fab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of differential expession analysis methods\n",
    "de_methods = [\"seurat\"]  # \"mast\"\n",
    "\n",
    "for de_method in de_methods:\n",
    "    # Make an empty list to store average of (preds - truth) of each purity levels\n",
    "    avg_diff_l = []\n",
    "    preds_l = []\n",
    "\n",
    "    # Iterate over purity levels\n",
    "    for pur_lvl in tqdm(purity_levels):\n",
    "        # Read and reorganize  index and columns to match truth_df\n",
    "        subset_preds_df = pd.read_csv(\n",
    "            Path(prefix).joinpath(\n",
    "                f\"data/dwls/results_{de_method}/{pur_lvl}/results.csv\"\n",
    "            ),\n",
    "            sep=\",\",\n",
    "            index_col=0,\n",
    "        ).T\n",
    "\n",
    "        subset_truth_df = truth_df[truth_df[\"Cancer_cell\"] == pur_lvl]\n",
    "\n",
    "        # Calcuate preds-truth for each purity level\n",
    "        diff_df = abs(\n",
    "            subset_preds_df[c_types].sort_index() - subset_truth_df.sort_index()\n",
    "        )\n",
    "        # diff_df = subset_preds_df[c_types] - subset_truth_df\n",
    "        avg_diff = diff_df.mean().to_frame()\n",
    "        avg_diff.columns = [pur_lvl]\n",
    "\n",
    "        avg_diff_l.append(avg_diff)\n",
    "        preds_l.append(subset_preds_df)\n",
    "\n",
    "    avg_diff_df = pd.concat(avg_diff_l, axis=1)\n",
    "    preds_df = pd.concat(preds_l, axis=0)\n",
    "\n",
    "    # Save predictions\n",
    "    preds_df.to_csv(\n",
    "        Path(prefix).joinpath(f\"data/results/dwls_{de_method}.tsv\"), sep=\"\\t\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f699990e",
   "metadata": {},
   "source": [
    "## MuSiC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d037fdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty list to store average of (preds - truth) of each purity levels\n",
    "avg_diff_l = []\n",
    "preds_l = []\n",
    "\n",
    "# Iterate over purity levels\n",
    "for pur_lvl in tqdm(purity_levels):\n",
    "    # Read and reorganize  index and columns to match truth_df\n",
    "    if expt == \"\":\n",
    "        res_path = Path(prefix).joinpath(f\"data/music/results/{pur_lvl}/results.csv\")\n",
    "    else:\n",
    "        res_path = Path(prefix).joinpath(\n",
    "            f\"data/music/results_{expt}/{pur_lvl}/results.csv\"\n",
    "        )\n",
    "\n",
    "    subset_preds_df = pd.read_csv(\n",
    "        res_path,\n",
    "        sep=\",\",\n",
    "        index_col=0,\n",
    "    )\n",
    "\n",
    "    subset_truth_df = truth_df[truth_df[\"Cancer_cell\"] == pur_lvl]\n",
    "\n",
    "    # Calcuate preds-truth for each purity level\n",
    "    diff_df = abs(subset_preds_df[c_types].sort_index() - subset_truth_df.sort_index())\n",
    "    # diff_df = subset_preds_df[c_types] - subset_truth_df\n",
    "    avg_diff = diff_df.mean().to_frame()\n",
    "    avg_diff.columns = [pur_lvl]\n",
    "\n",
    "    avg_diff_l.append(avg_diff)\n",
    "    preds_l.append(subset_preds_df)\n",
    "\n",
    "avg_diff_df = pd.concat(avg_diff_l, axis=1)\n",
    "preds_df = pd.concat(preds_l, axis=0)\n",
    "\n",
    "# Save predictions\n",
    "preds_df.to_csv(Path(prefix).joinpath(f\"data/results/music.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8638ce88",
   "metadata": {},
   "source": [
    "## hspe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97b8a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty list to store average of (preds - truth) of each purity levels\n",
    "avg_diff_l = []\n",
    "preds_l = []\n",
    "\n",
    "# Iterate over purity levels\n",
    "for pur_lvl in tqdm(purity_levels):\n",
    "\n",
    "    # Iterate over each of the 20 partitions\n",
    "    for partition in list(range(0, 20, 1)):\n",
    "\n",
    "        # Read and reorganize  index and columns to match truth_df\n",
    "        if expt == \"\":\n",
    "            res_path = Path(prefix).joinpath(\n",
    "                f\"data/hspe/results/{pur_lvl}/{partition}/results.csv\"\n",
    "            )\n",
    "        else:\n",
    "            res_path = Path(prefix).joinpath(\n",
    "                f\"data/hspe/results_{expt}/{pur_lvl}/{partition}/results.csv\"\n",
    "            )\n",
    "\n",
    "        subset_preds_df = pd.read_csv(\n",
    "            res_path,\n",
    "            sep=\",\",\n",
    "            index_col=0,\n",
    "        )\n",
    "\n",
    "        preds_l.append(subset_preds_df)\n",
    "\n",
    "preds_df = pd.concat(preds_l, axis=0)\n",
    "\n",
    "# Calcuate preds-truth for each purity level\n",
    "avg_diff_l = []\n",
    "for pur_lvl in tqdm(purity_levels):\n",
    "    subset_truth_df = truth_df[truth_df[\"Cancer_cell\"] == pur_lvl]\n",
    "    subset_preds_df = preds_df[preds_df.index.isin(subset_truth_df.index)]\n",
    "\n",
    "    diff_df = abs(subset_preds_df[c_types].sort_index() - subset_truth_df.sort_index())\n",
    "    avg_diff = diff_df.mean().to_frame()\n",
    "    avg_diff.columns = [pur_lvl]\n",
    "    avg_diff_l.append(avg_diff)\n",
    "\n",
    "# Save results to csv\n",
    "preds_df.to_csv(Path(prefix).joinpath(f\"data/results/hspe.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e756e525",
   "metadata": {},
   "source": [
    "## BayesPrism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758b0b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty list to store average of (preds - truth) of each purity levels\n",
    "avg_diff_l = []\n",
    "preds_l = []\n",
    "\n",
    "# We run BayesPrism v2 in different experiments. Decided which set of results to pull\n",
    "# Experiment name will also be results file's suffix\n",
    "expt = \"no_marker_genes\"  # \"marker_genes_cell_states\"\n",
    "\n",
    "# Iterate over purity levels\n",
    "for pur_lvl in tqdm(purity_levels):\n",
    "    # Read and reorganize  index and columns to match truth_df\n",
    "    subset_preds_df = pd.read_csv(\n",
    "        Path(prefix).joinpath(f\"data/bprism/results_{expt}/{pur_lvl}/results.csv\"),\n",
    "        sep=\",\",\n",
    "        index_col=0,\n",
    "    )\n",
    "\n",
    "    subset_truth_df = truth_df[truth_df[\"Cancer_cell\"] == pur_lvl]\n",
    "\n",
    "    # Calcuate preds-truth for each purity level\n",
    "    diff_df = abs(subset_preds_df[c_types].sort_index() - subset_truth_df.sort_index())\n",
    "    # diff_df = subset_preds_df[c_types] - subset_truth_df\n",
    "    avg_diff = diff_df.mean().to_frame()\n",
    "    avg_diff.columns = [pur_lvl]\n",
    "\n",
    "    avg_diff_l.append(avg_diff)\n",
    "    preds_l.append(subset_preds_df)\n",
    "\n",
    "avg_diff_df = pd.concat(avg_diff_l, axis=1)\n",
    "preds_df = pd.concat(preds_l, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd86ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions\n",
    "preds_df.to_csv(Path(prefix).joinpath(f\"data/results/bprism_{expt}.tsv\"), sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e21e3183055704cdc6beb302a7eaad42e1c0671a451dc4bde87185c59632390"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
