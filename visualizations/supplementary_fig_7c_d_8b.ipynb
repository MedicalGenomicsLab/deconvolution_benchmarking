{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b7cc55-3b46-46c2-bdd6-6e34ffdc37f1",
   "metadata": {},
   "source": [
    "# Visualize models performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a63a1-8e52-493c-b685-5a419fa97191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as adata\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly as plotly\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity as skl_cosine\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import braycurtis, cdist\n",
    "from math import sqrt\n",
    "\n",
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68eb651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set prefixes\n",
    "prefix = \"???/deconvolution_benchmarking/05_external_scrna_validation/pal_et_al\"\n",
    "\n",
    "# Prefix to visualizations folder\n",
    "viz_prefix = \"???/deconvolution_benchmarking/visualizations\"\n",
    "\n",
    "# Major cell types\n",
    "c_types = [\n",
    "    \"Cancer_epithelial\",\n",
    "    # \"Cycling epithelial\",\n",
    "    \"Normal_epithelial\",\n",
    "    \"T_cells\",\n",
    "    \"B_cells\",\n",
    "    \"Myeloid\",\n",
    "    \"TAMs\",\n",
    "    \"DCs\",\n",
    "    \"Endothelial\",\n",
    "    \"CAFs\",\n",
    "    \"Pericytes\",\n",
    "    \"Plasma_cells\",\n",
    "]\n",
    "# List tumour purity levels\n",
    "purity_levels = np.arange(0.05, 1, 0.05).round(3).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f4012",
   "metadata": {},
   "source": [
    "### Load groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load truth.csv\n",
    "truth_df = pd.read_csv(\n",
    "    Path(prefix).joinpath(\"data/results/truth.tsv\"), sep=\"\\t\", index_col=0\n",
    ")\n",
    "truth_df = truth_df[c_types]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4722caf",
   "metadata": {},
   "source": [
    "## [Fig]. RMSE over tumour purity level across cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d2065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need to show all 19 tumour purities for each tool. Just half of it will be fine\n",
    "# Getting tumour purity levels with intervals of 15% instead of 10%\n",
    "reduced_purity_levels = np.arange(0.05, 1, 0.15).round(3).tolist()\n",
    "\n",
    "methods = [\n",
    "    \"bprism_v2_no_marker_genes\",\n",
    "    \"scaden\",\n",
    "    \"music\",\n",
    "    \"bisque\",\n",
    "    \"cbx\",\n",
    "    \"hspe\",\n",
    "    \"epic\",\n",
    "    \"dwls_seurat\",\n",
    "]\n",
    "\n",
    "ctypes_order = [\n",
    "    \"Plasma_cells\",\n",
    "    \"Pericytes\",\n",
    "    \"CAFs\",\n",
    "    \"Endothelial\",\n",
    "    \"DCs\",\n",
    "    \"TAMs\",\n",
    "    \"Myeloid\",\n",
    "    \"B_cells\",\n",
    "    \"T_cells\",\n",
    "    \"Normal_epithelial\",\n",
    "    \"Cancer_epithelial\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(\n",
    "    subset_truth_df: pd.DataFrame, subset_res_df: pd.DataFrame, c_types: List\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Iterate over provided cell types and calculate peformance metrics of predictions against groundtruth\n",
    "    The method assumes that provided cell types are consistent across both prediction and groundtruth DataFrame\n",
    "\n",
    "    Args:\n",
    "        - subset_truth_df:     groundtruth DataFrame, purity-level-specific\n",
    "        - subset_res_df:     predictions DataFrame, purity-level-specific\n",
    "        - c_types:             cell types to iterate over\n",
    "    \"\"\"\n",
    "    # Create an empty list to hold peformance metrics of each cell type\n",
    "    metrics_series_l = []\n",
    "\n",
    "    # Iterate over cell types and calcuate RMSE + MAE + Cosine\n",
    "    for c_type in c_types:\n",
    "        # Re-arrange colums both predictions and groundtruth in the same order\n",
    "        ctype_truth_df = subset_truth_df[c_type]\n",
    "        ctype_preds_df = subset_res_df[c_type]\n",
    "\n",
    "        # RMSE\n",
    "        rmse = sqrt(mean_squared_error(ctype_truth_df * 100, ctype_preds_df * 100))\n",
    "\n",
    "        # MAE\n",
    "        mae = abs(ctype_truth_df - ctype_preds_df).median() * 100\n",
    "\n",
    "        # RPE\n",
    "        rpe = (\n",
    "            abs(ctype_truth_df - ctype_preds_df) / ctype_truth_df.replace({0: 0.0001})\n",
    "        ).median()\n",
    "\n",
    "        metrics_series_l.append(pd.Series([rmse, mae, rpe], name=c_type))\n",
    "\n",
    "    # Concatenate metrics across cell types\n",
    "    method_metrics_df = pd.concat(metrics_series_l, axis=1)\n",
    "    method_metrics_df.index = [\"RMSE\", \"MAE\", \"RPE\"]\n",
    "\n",
    "    return method_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad621bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rmse_heatmap(\n",
    "    avg_diff_df: pd.DataFrame,\n",
    "    outfile_name: str,\n",
    "    metric: str,\n",
    "    metric_suffix: str,\n",
    "    colorscale: str,\n",
    "    c_types: List,\n",
    "    plot_w: int,\n",
    "    plot_h: int,\n",
    "    z_range: List = [0, 50],\n",
    "    dticks: int = 10,\n",
    "    auto_open: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Plot heatmap of Mean Absolute Error across tumour purity levels\n",
    "\n",
    "    Args:\n",
    "        - avg_diff_df:        DataFrame holding MAE over tumour purity levels\n",
    "        - outfile_name:       name of output html and png files\n",
    "        - c_types:            cell types in a specific order we'd like to appear on the y-axis\n",
    "        - z_range:            Maximum error (between Scaden, CBX and EPIC) is ~56%\n",
    "                              so we only need to set maximum zaxis to 50%.\n",
    "                              This ensure extreme errors are very red on the scale\n",
    "        - auto_open:          Whether to open html after creation or not\n",
    "\n",
    "    \"\"\"\n",
    "    # Create annotated heatmap object with plotly\n",
    "    fig = ff.create_annotated_heatmap(\n",
    "        z=avg_diff_df.values,\n",
    "        # Annotate each cell in the heatmap with the corresponding labels\n",
    "        annotation_text=avg_diff_df.values.round(2).astype(str),\n",
    "        zmin=z_range[0],\n",
    "        zmax=z_range[1],\n",
    "        x=(avg_diff_df.columns * 100).astype(int).tolist(),  # Rows are purity levels\n",
    "        y=avg_diff_df.index.tolist(),  # Columns are cell types\n",
    "        colorscale=colorscale,\n",
    "        showscale=False,\n",
    "        hoverinfo=\"text\",\n",
    "        text=avg_diff_df.values.round(4),\n",
    "        colorbar=dict(\n",
    "            title=metric,\n",
    "            ticks=\"outside\",\n",
    "            ticksuffix=metric_suffix,\n",
    "            dtick=dticks,\n",
    "            orientation=\"h\",\n",
    "            ticklen=2,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Update axes\n",
    "    fig.update_xaxes(\n",
    "        title=\"Tumour purity levels (%)\",\n",
    "        title_font_size=8,\n",
    "        title_standoff=5,\n",
    "        ticks=\"outside\",\n",
    "        tickfont_size=7,\n",
    "        showticklabels=True,\n",
    "        ticklen=2,\n",
    "        tickwidth=0.5,\n",
    "        tickmode=\"array\",\n",
    "        tickvals=(avg_diff_df.columns * 100).astype(int).tolist(),\n",
    "        linecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "        side=\"bottom\",\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        title=\"Cell Types\",\n",
    "        title_font_size=8,\n",
    "        title_standoff=5,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "        categoryorder=\"array\",\n",
    "        categoryarray=c_types,  # Order cell types by linages\n",
    "        ticks=\"outside\",\n",
    "        showticklabels=True,\n",
    "        ticklen=2,\n",
    "        tickwidth=0.5,\n",
    "        tickfont_size=7,\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig[\"layout\"].update(\n",
    "        margin=dict(t=0, l=0, r=0, b=0),\n",
    "        font_size=6,\n",
    "        plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        font_color=\"black\",\n",
    "    )\n",
    "\n",
    "    # Save offline mode\n",
    "    fig.write_image(\n",
    "        Path(f\"{outfile_name}\").with_suffix(\".svg\"),\n",
    "        width=plot_w,\n",
    "        height=plot_h,\n",
    "        scale=5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53a1c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes to attach to Sunburst plot for each metrics\n",
    "metrics_mapping = {\n",
    "    \"MAE\": {\n",
    "        \"colorscale\": \"purples\",\n",
    "        \"zmin\": 0,\n",
    "        \"zmax\": 50,\n",
    "        \"title\": \"Mean Absolute Error\",\n",
    "        \"tick_suffix\": \"\",\n",
    "        \"d_tick\": 10,\n",
    "    },\n",
    "    \"RMSE\": {\n",
    "        \"colorscale\": \"reds\",\n",
    "        \"zmin\": 0,\n",
    "        \"zmax\": 50,\n",
    "        \"title\": \"RMSE\",\n",
    "        \"tick_suffix\": \"\",\n",
    "        \"d_tick\": 10,\n",
    "    },\n",
    "    \"RPE\": {\n",
    "        \"colorscale\": \"oranges\",\n",
    "        \"zmin\": 0,\n",
    "        \"zmax\": 1000,\n",
    "        \"title\": \"RPE\",\n",
    "        \"tick_suffix\": \" %\",\n",
    "        \"d_tick\": 100,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f35ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rmse_l = []\n",
    "\n",
    "# For each tool, calculate average absolute error across tumour purity levels\n",
    "# for method in tqdm(methods):\n",
    "for method in methods:\n",
    "    print(f\"Generating performance metrics for {method}\")\n",
    "    res_df = pd.read_csv(\n",
    "        Path(prefix).joinpath(f\"data/results/{method}.tsv\"), sep=\"\\t\", index_col=0\n",
    "    )\n",
    "\n",
    "    # Some values can be ~-0.00001..., replace them by 0\n",
    "    res_df.clip(lower=0, inplace=True)\n",
    "\n",
    "    # Rearrange columns to match truth_df\n",
    "    res_df = res_df[truth_df.columns]\n",
    "\n",
    "    # Empty list to hold metrics\n",
    "    all_metrics_l = []\n",
    "\n",
    "    for pur_lvl in tqdm(purity_levels):\n",
    "        # Calculate average of the absolute difference for each purity level\n",
    "        subset_truth_df = truth_df[\n",
    "            truth_df[\"Cancer_epithelial\"] == pur_lvl\n",
    "        ].sort_index()\n",
    "        subset_res_df = res_df[res_df.index.isin(subset_truth_df.index)].sort_index()\n",
    "\n",
    "        assert subset_res_df.index.equals(subset_truth_df.index)\n",
    "        assert subset_res_df.columns.equals(subset_truth_df.columns)\n",
    "\n",
    "        pur_lvl_metrics_df = calculate_metrics(\n",
    "            subset_truth_df=subset_truth_df,\n",
    "            subset_res_df=subset_res_df,\n",
    "            c_types=c_types,\n",
    "        )\n",
    "        pur_lvl_metrics_df[\"Purity Level\"] = pur_lvl\n",
    "        all_metrics_l.append(pur_lvl_metrics_df)\n",
    "\n",
    "    # Concatenate\n",
    "    all_metrics_df = pd.concat(all_metrics_l, axis=0)\n",
    "    all_metrics_df = all_metrics_df.round(2)\n",
    "\n",
    "    # Plot each metric separately\n",
    "    for metric in [\"RMSE\"]:\n",
    "        # Plot reduced tumour purity levels\n",
    "        reduced_pur_lvl_df = (\n",
    "            all_metrics_df.loc[\n",
    "                (all_metrics_df[\"Purity Level\"].isin(reduced_purity_levels))\n",
    "                & (all_metrics_df.index == \"RMSE\")\n",
    "            ]\n",
    "            .set_index([\"Purity Level\"])\n",
    "            .T\n",
    "        )\n",
    "\n",
    "        plot_metrics_df = all_metrics_df.loc[metric].set_index([\"Purity Level\"]).T\n",
    "        plot_rmse_heatmap(\n",
    "            avg_diff_df=reduced_pur_lvl_df,\n",
    "            outfile_name=f\"production/supp_figures/supp_fig_8b_{metric}_{method}\",\n",
    "            metric=metric,\n",
    "            metric_suffix=metrics_mapping[metric][\"tick_suffix\"],\n",
    "            c_types=ctypes_order,\n",
    "            z_range=[metrics_mapping[metric][\"zmin\"], metrics_mapping[metric][\"zmax\"]],\n",
    "            dticks=metrics_mapping[metric][\"d_tick\"],\n",
    "            colorscale=metrics_mapping[metric][\"colorscale\"],\n",
    "            plot_w=250,\n",
    "            plot_h=125,\n",
    "        )\n",
    "\n",
    "        # Collect RMSE and method\n",
    "        rmse_df = (\n",
    "            all_metrics_df.loc[\"RMSE\"].reset_index().rename(columns={\"index\": \"metric\"})\n",
    "        )\n",
    "        rmse_df[\"method\"] = method\n",
    "        all_rmse_l.append(rmse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdee8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save source data\n",
    "all_rmse_df = pd.concat(all_rmse_l, axis=0)\n",
    "all_rmse_df[\"method\"].replace(\n",
    "    {\n",
    "        \"scaden\": \"Scaden\",\n",
    "        \"music\": \"MuSiC\",\n",
    "        \"cbx\": \"CBX\",\n",
    "        \"bisque\": \"Bisque\",\n",
    "        \"dwls_seurat\": \"DWLS\",\n",
    "        \"epic\": \"EPIC\",\n",
    "        \"cpm\": \"CPM\",\n",
    "        \"bprism_v2_no_marker_genes\": \"BayesPrism\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "all_rmse_df.to_csv(Path(viz_prefix).joinpath(\"data/supp_figure_8b.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261e80ef",
   "metadata": {},
   "source": [
    "## [Fig]. Bray Curtis dissimilarity across tumour purity levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada78343",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\n",
    "    \"bprism_v2_no_marker_genes\",\n",
    "    \"scaden\",\n",
    "    \"music\",\n",
    "    \"cbx\",\n",
    "    \"hspe\",\n",
    "    \"epic\",\n",
    "    \"bisque\",\n",
    "    \"dwls_seurat\",\n",
    "]\n",
    "\n",
    "# We don't need to show all 19 tumour purities for each tool. Just half of it will be fine\n",
    "# Getting tumour purity levels with intervals of 15% instead of 10%\n",
    "reduced_purity_levels = np.arange(0.05, 1, 0.15).round(3).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92801f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bray_curtis_l = []\n",
    "\n",
    "for method in tqdm(methods):\n",
    "    res_df = pd.read_csv(\n",
    "        Path(prefix).joinpath(f\"data/results/{method}.tsv\"), sep=\"\\t\", index_col=0\n",
    "    )\n",
    "    res_df = res_df[c_types]\n",
    "\n",
    "    # Clip tiny negative numbers to 0\n",
    "    res_df.clip(lower=0, inplace=True)\n",
    "\n",
    "    # Check if indexes match\n",
    "    assert (res_df.sort_index().index == truth_df.sort_index().index).all()\n",
    "    assert (res_df.sort_index().columns == truth_df.sort_index().columns).all()\n",
    "\n",
    "    # Iterate over res_df and calculate Bray-Curtis index\n",
    "    for sample_id in res_df.index:\n",
    "        bray_curtis_dissi = braycurtis(res_df.loc[sample_id], truth_df.loc[sample_id])\n",
    "        bray_curtis_l.append(\n",
    "            (\n",
    "                sample_id,\n",
    "                bray_curtis_dissi,\n",
    "                truth_df.loc[sample_id, \"Cancer_epithelial\"],\n",
    "                method,\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Concatenate all rmse dataframes\n",
    "bray_curtis_df = pd.DataFrame(\n",
    "    bray_curtis_l, columns=[\"Mixture ID\", \"Bray Curtis Dissi\", \"Purity Level\", \"Method\"]\n",
    ").set_index([\"Mixture ID\"])\n",
    "\n",
    "# Rename method names\n",
    "bray_curtis_df.replace(\n",
    "    {\n",
    "        \"scaden\": \"Scaden\",\n",
    "        \"music\": \"MuSiC\",\n",
    "        \"cbx\": \"CBX\",\n",
    "        \"bisque\": \"Bisque\",\n",
    "        \"dwls_seurat\": \"DWLS\",\n",
    "        \"epic\": \"EPIC\",\n",
    "        \"cpm\": \"CPM\",\n",
    "        \"bprism_v2_no_marker_genes\": \"BayesPrism\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Convert purity level to % scale and categorical\n",
    "bray_curtis_df[\"Purity Level\"] = (bray_curtis_df[\"Purity Level\"] * 100).astype(int)\n",
    "bray_curtis_df[\"Purity Level\"] = bray_curtis_df[\"Purity Level\"].astype(\"category\")\n",
    "bray_curtis_df[\"Purity Level\"].cat.reorder_categories(\n",
    "    [int(i * 100) for i in purity_levels], inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47773868",
   "metadata": {},
   "source": [
    "#### Boxplots of Bray Curtis dissimilarity across tumour purity levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5170b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only reduced tumour purity levels\n",
    "reduced_bray_curtis_df = bray_curtis_df[\n",
    "    bray_curtis_df[\"Purity Level\"].isin(int(i * 100) for i in reduced_purity_levels)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c7582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save source data\n",
    "reduced_bray_curtis_df.to_csv(\n",
    "    Path(viz_prefix).joinpath(\"data/supp_figure_7c.tsv\"), sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8a2da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(\n",
    "    reduced_bray_curtis_df,\n",
    "    x=\"Method\",\n",
    "    y=\"Bray Curtis Dissi\",\n",
    "    color=\"Purity Level\",\n",
    "    color_discrete_sequence=px.colors.sequential.Greys[-len(reduced_purity_levels) :],\n",
    "    notched=True,\n",
    "    category_orders={\n",
    "        \"Method\": [\n",
    "            \"BayesPrism\",\n",
    "            \"Scaden\",\n",
    "            \"MuSiC\",\n",
    "            \"hspe\",\n",
    "            \"DWLS\",\n",
    "            \"CBX\",\n",
    "            \"Bisque\",\n",
    "            \"EPIC\",\n",
    "            \"CPM\",\n",
    "        ]\n",
    "    },\n",
    ")\n",
    "\n",
    "# Update dot sizes\n",
    "for i, pur_lvl in enumerate([int(i * 100) for i in reduced_purity_levels]):\n",
    "    fig.update_traces(\n",
    "        marker=dict(size=1.5),\n",
    "        line=dict(width=0.5, color=\"DarkSlateGray\"),\n",
    "        fillcolor=px.colors.sequential.gray_r[i],  # plus 2 to avoid white\n",
    "        opacity=1,\n",
    "        selector=dict(name=str(pur_lvl)),\n",
    "    )\n",
    "\n",
    "fig[\"layout\"].update(\n",
    "    boxmode=\"group\",\n",
    "    boxgroupgap=0.2,\n",
    "    plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "    showlegend=False,\n",
    "    font_size=10,\n",
    "    font_color=\"black\",\n",
    "    xaxis=dict(\n",
    "        title=\"Method\",\n",
    "        title_font_size=8,\n",
    "        title_standoff=5,\n",
    "        ticks=\"outside\",\n",
    "        showticklabels=True,\n",
    "        tickmode=\"array\",\n",
    "        tickfont_size=7,\n",
    "        ticklen=2,\n",
    "        tickangle=0,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=0.75,\n",
    "        side=\"bottom\",\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Bray-Curtis Dissimilarity\",\n",
    "        title_font_size=8,\n",
    "        title_standoff=4,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=0.75,\n",
    "        tickfont_size=7,\n",
    "        ticklen=2,\n",
    "        ticks=\"outside\",\n",
    "        showticklabels=True,\n",
    "        showgrid=True,\n",
    "        gridwidth=0.75,\n",
    "        gridcolor=\"lightgray\",\n",
    "        range=[0, 1],\n",
    "        dtick=0.1,\n",
    "    ),\n",
    "    legend=dict(title=dict(text=\"Purity level (%)\", font_size=8), font_size=8),\n",
    "    margin=dict(t=0, l=0, r=0, b=0),\n",
    ")\n",
    "\n",
    "# Save svg\n",
    "fig.write_image(\n",
    "    Path(f\"production/supp_figures/supp_fig_7c\").with_suffix(\".svg\"),\n",
    "    scale=5,\n",
    "    # height=200,\n",
    "    # width=350,\n",
    "    height=250,\n",
    "    width=875,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87708bad",
   "metadata": {},
   "source": [
    "#### Heatmaps of median Bray Curtis dissimilarity across tumour purity levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get median of bray-curtis at each tumour purity level\n",
    "median_bray_curtis_df = bray_curtis_df.groupby([\"Method\", \"Purity Level\"]).agg(\n",
    "    [\"median\"]\n",
    ")\n",
    "\n",
    "# 1st level of multi-index column is redundant, drop it and reset index\n",
    "median_bray_curtis_df = median_bray_curtis_df.droplevel(0, axis=1).reset_index()\n",
    "\n",
    "# Pivot wider so we have purity levels as columns (x-axis) and methods as rows (y-axis)\n",
    "pivot_median_bray_curtis_df = median_bray_curtis_df.pivot(\n",
    "    index=\"Method\", columns=\"Purity Level\", values=\"median\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882a84b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_order = [\n",
    "    \"EPIC\",\n",
    "    \"Bisque\",\n",
    "    \"DWLS\",\n",
    "    \"CBX\",\n",
    "    \"hspe\",\n",
    "    \"MuSiC\",\n",
    "    \"Scaden\",\n",
    "    \"BayesPrism\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa74a228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save source data\n",
    "median_bray_curtis_df.rename(\n",
    "    columns={\"median\": \"Meidan Bray-Curtis Dissimilarity\"}\n",
    ").to_csv(Path(viz_prefix).joinpath(\"data/supp_figure_7d.tsv\"), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1943250",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_median_bray_curtis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c745e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ff.create_annotated_heatmap(\n",
    "    z=pivot_median_bray_curtis_df.values,\n",
    "    annotation_text=pivot_median_bray_curtis_df.values.round(2),\n",
    "    zmin=0,\n",
    "    zmax=1,\n",
    "    x=pivot_median_bray_curtis_df.columns.tolist(),\n",
    "    y=pivot_median_bray_curtis_df.index.tolist(),\n",
    "    colorscale=\"teal\",\n",
    "    showscale=True,\n",
    "    hoverinfo=\"text\",\n",
    "    text=pivot_median_bray_curtis_df.round(2).values,\n",
    "    colorbar=dict(\n",
    "        title=\"Bray-Curis<br>Dissimilarity\",\n",
    "        titlefont_size=14,\n",
    "        ticks=\"outside\",\n",
    "        ticksuffix=\"\",\n",
    "        tickfont_size=12,\n",
    "        dtick=0.1,\n",
    "        orientation=\"v\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig[\"layout\"].update(\n",
    "    margin=dict(t=0, l=0, r=0, b=0),\n",
    "    plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "    font=dict(size=12, color=\"black\"),\n",
    "    xaxis=dict(\n",
    "        title=\"Tumour purity levels (%)\",\n",
    "        titlefont_size=14,\n",
    "        title_standoff=5,\n",
    "        ticks=\"outside\",\n",
    "        showticklabels=True,\n",
    "        tickmode=\"array\",\n",
    "        tickwidth=0.75,\n",
    "        ticklen=3,\n",
    "        tickvals=[int(i * 100) for i in purity_levels],\n",
    "        tickfont_size=12,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=0.75,\n",
    "        side=\"bottom\",\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Methods\",\n",
    "        titlefont_size=14,\n",
    "        title_standoff=1,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=0.75,\n",
    "        ticks=\"outside\",\n",
    "        tickwidth=0.75,\n",
    "        ticklen=3,\n",
    "        tickfont_size=12,\n",
    "        categoryorder=\"array\",\n",
    "        categoryarray=methods_order,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Save into svg\n",
    "fig.write_image(\n",
    "    Path(viz_prefix)\n",
    "    .joinpath(\"production/supp_figures/supp_fig_7d\")\n",
    "    .with_suffix(\".svg\"),\n",
    "    width=900,\n",
    "    height=300,\n",
    "    scale=5,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e21e3183055704cdc6beb302a7eaad42e1c0671a451dc4bde87185c59632390"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
