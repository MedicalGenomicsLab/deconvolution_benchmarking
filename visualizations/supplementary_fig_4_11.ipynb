{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b7cc55-3b46-46c2-bdd6-6e34ffdc37f1",
   "metadata": {},
   "source": [
    "# Visualize models performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a63a1-8e52-493c-b685-5a419fa97191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as adata\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly as plotly\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity as skl_cosine\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import cosine as scipy_cosine\n",
    "from scipy.spatial.distance import braycurtis, cdist\n",
    "from math import sqrt\n",
    "\n",
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52cef7f-433e-4db0-8f5f-942d374522df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefix to visualizations folder\n",
    "viz_prefix = \"???/deconvolution_benchmarking/visualizations\"\n",
    "\n",
    "# Prefix to the experiment we're plotting\n",
    "prefix = \"???/deconvolution_benchmarking/01_purity_levels_experiment/include_normal_epithelial\"\n",
    "\n",
    "# Tumour purity levels\n",
    "purity_levels = np.arange(0.05, 1, 0.05).round(3).tolist()\n",
    "\n",
    "# Major cell types\n",
    "c_types = [\n",
    "    \"Cancer Epithelial\",\n",
    "    \"Normal Epithelial\",\n",
    "    \"T-cells\",\n",
    "    \"B-cells\",\n",
    "    \"Myeloid\",\n",
    "    \"CAFs\",\n",
    "    \"Endothelial\",\n",
    "    \"PVL\",\n",
    "    \"Plasmablasts\",\n",
    "]\n",
    "\n",
    "# Methods order are universal across figures\n",
    "methods_order = [\n",
    "    \"BayesPrism\",\n",
    "    \"Scaden\",\n",
    "    \"MuSiC\",\n",
    "    \"hspe\",\n",
    "    \"DWLS\",\n",
    "    \"CBX\",\n",
    "    \"Bisque\",\n",
    "    \"EPIC\",\n",
    "    \"CPM\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77080247",
   "metadata": {},
   "source": [
    "### Load groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f562086a-5077-43a2-989e-422ee88109af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load truth.csv\n",
    "truth_df = pd.read_csv(\n",
    "    Path(prefix).joinpath(\"data/results/truth.csv\"), sep=\"\\t\", index_col=0\n",
    ")\n",
    "truth_df = truth_df[c_types]\n",
    "\n",
    "# Pivot longer for when we need it\n",
    "truth_copy_df = truth_df.copy().sample(frac=0.05, random_state=41)\n",
    "truth_copy_df[\"purity_level\"] = truth_copy_df[\"Cancer Epithelial\"]\n",
    "\n",
    "pivot_truth_df = (\n",
    "    truth_copy_df.reset_index()\n",
    "    .melt(id_vars=[\"index\", \"purity_level\"], value_vars=c_types)\n",
    "    .rename(columns={\"index\": \"mixture_id\", \"variable\": \"cell_type\", \"value\": \"truth\"})\n",
    "    .set_index([\"mixture_id\", \"cell_type\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3565677a",
   "metadata": {},
   "source": [
    "### Extract colour pallete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f8f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract colour pallete\n",
    "ctype_colour_pallete_df = pd.read_csv(\n",
    "    Path(prefix).joinpath(\"data/Whole_miniatlas_colour_pallete.csv\"), sep=\"\\t\"\n",
    ")\n",
    "\n",
    "# Convert to dictionary\n",
    "ctype_colour_pallete_d = {\n",
    "    row[\"all_celltype\"]: {\"fill\": row[\"fill\"], \"line\": row[\"line\"]}\n",
    "    for i, row in ctype_colour_pallete_df.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb775e0c",
   "metadata": {},
   "source": [
    "## [Supp Fig 4b]. Pearson's r over tumour purity levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b33130",
   "metadata": {},
   "source": [
    "### Collect results and calculate metrics\n",
    "All (predictions-groundtruth) DataFrames are stored in the same format [Mixtures x Cell_types] for all methods <br>\n",
    "Specific which methods we'd like to collect results for<br>\n",
    "The collect results and generate performance metrics nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee87df",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\n",
    "    \"hspe\",\n",
    "    \"music\",\n",
    "    \"cpm\",\n",
    "    \"cbx\",\n",
    "    \"scaden\",\n",
    "    \"epic\",\n",
    "    \"bisque\",\n",
    "    \"dwls\",\n",
    "    \"bprism_v2\",\n",
    "]\n",
    "\n",
    "pearsonr_l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a49b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in tqdm(methods):\n",
    "    res_df = pd.read_csv(\n",
    "        Path(prefix).joinpath(f\"data/results/{method}.csv\"), sep=\"\\t\", index_col=0\n",
    "    )\n",
    "    res_df = res_df[c_types]\n",
    "    res_df[res_df < 0] = 0\n",
    "\n",
    "    print(f\"Generating performance metrics for {method}\")\n",
    "\n",
    "    # Iterate over purity levels and calculate performance metrices for each level\n",
    "    pearrson_d = {}\n",
    "\n",
    "    for pur_lvl in tqdm(purity_levels):\n",
    "        subset_truth_df = truth_df[\n",
    "            truth_df[\"Cancer Epithelial\"] == pur_lvl\n",
    "        ].sort_index()\n",
    "        subset_res_df = res_df[res_df.index.isin(subset_truth_df.index)].sort_index()\n",
    "\n",
    "        # Calculate Pearson's r\n",
    "        pearson_r = pearsonr(\n",
    "            subset_truth_df.values.flatten(), subset_res_df.values.flatten()\n",
    "        )\n",
    "        pearrson_d[pur_lvl] = pearson_r[0]\n",
    "\n",
    "    pearson_series = pd.Series(pearrson_d)\n",
    "    pearson_series.name = method\n",
    "    pearsonr_l.append(pearson_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all metrics across purity levels into total metrics DataFrames\n",
    "pearsonr_df = pd.concat(pearsonr_l, axis=1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1728472e",
   "metadata": {},
   "source": [
    "#### Pivot the metrics that we'd like to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b53ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson's r\n",
    "pivot_pearsonr_df = pd.melt(\n",
    "    pearsonr_df.reset_index(), id_vars=[\"index\"], value_vars=purity_levels\n",
    ")\n",
    "pivot_pearsonr_df.columns = [\"Method\", \"Purity Level\", \"Pearson's r\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71486ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merger all metrics into one beautiful metrics DataFrame\n",
    "metrics_df = pivot_pearsonr_df.copy()\n",
    "metrics_df[\"Pearsonr_round\"] = metrics_df[\"Pearson's r\"].round(2)\n",
    "\n",
    "# Replace tools by their correct names\n",
    "metrics_df.replace(\n",
    "    {\n",
    "        \"scaden\": \"Scaden\",\n",
    "        \"music\": \"MuSiC\",\n",
    "        \"cbx\": \"CBX\",\n",
    "        \"bisque\": \"Bisque\",\n",
    "        \"dwls\": \"DWLS\",\n",
    "        \"epic\": \"EPIC\",\n",
    "        \"cpm\": \"CPM\",\n",
    "        \"bprism_v2\": \"BayesPrism\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Convert purity level to % scale and categorical\n",
    "metrics_df[\"Purity Level\"] = (metrics_df[\"Purity Level\"] * 100).astype(int)\n",
    "metrics_df[\"Purity Level\"] = metrics_df[\"Purity Level\"].astype(\"category\")\n",
    "metrics_df[\"Purity Level\"].cat.reorder_categories(\n",
    "    [int(i * 100) for i in purity_levels], inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57cbfb6",
   "metadata": {},
   "source": [
    "### Plot heatmap of Pearson's r over tumour purity levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b72820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save source data\n",
    "metrics_df[[\"Method\", \"Purity Level\", \"Pearson's r\"]].to_csv(\n",
    "    Path(viz_prefix).joinpath(\"source_data/supp_figure_4a.tsv\"), sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1646c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_pearson_df = pd.pivot_table(\n",
    "    metrics_df[[\"Method\", \"Purity Level\", \"Pearson's r\"]],\n",
    "    values=\"Pearson's r\",\n",
    "    index=[\"Method\"],\n",
    "    columns=[\"Purity Level\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cdb9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ff.create_annotated_heatmap(\n",
    "    z=pivot_pearson_df.values,\n",
    "    annotation_text=pivot_pearson_df.values.round(2),  # Annotate with Pearson's r\n",
    "    zmin=0,\n",
    "    zmax=1,\n",
    "    x=pivot_pearson_df.columns.tolist(),  # Rows are methods\n",
    "    y=pivot_pearson_df.index.tolist(),  # Columns are cell types\n",
    "    colorscale=\"blues_r\",\n",
    "    showscale=True,\n",
    "    hoverinfo=\"text\",\n",
    "    text=pivot_pearson_df.round(2).values,\n",
    "    colorbar=dict(\n",
    "        title=\"Pearson's r\",\n",
    "        titlefont_size=14,\n",
    "        ticks=\"outside\",\n",
    "        ticksuffix=\"\",\n",
    "        tickfont_size=12,\n",
    "        dtick=0.1,\n",
    "        orientation=\"v\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig[\"layout\"].update(\n",
    "    margin=dict(t=0, l=0, r=0, b=0),\n",
    "    plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "    font=dict(size=12, color=\"black\"),\n",
    "    xaxis=dict(\n",
    "        title=\"Tumour purity levels\",\n",
    "        titlefont_size=14,\n",
    "        title_standoff=5,\n",
    "        ticks=\"outside\",\n",
    "        showticklabels=True,\n",
    "        tickmode=\"array\",\n",
    "        tickwidth=0.75,\n",
    "        ticklen=3,\n",
    "        tickvals=[int(i * 100) for i in purity_levels],\n",
    "        tickfont_size=12,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=0.75,\n",
    "        side=\"bottom\",\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Methods\",\n",
    "        titlefont_size=14,\n",
    "        title_standoff=1,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=0.75,\n",
    "        ticks=\"outside\",\n",
    "        tickwidth=0.75,\n",
    "        ticklen=3,\n",
    "        tickfont_size=12,\n",
    "        categoryorder=\"array\",\n",
    "        categoryarray=methods_order[::-1],\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Save into PNG and SVG\n",
    "fig.write_image(\n",
    "    Path(\"figures/supp_figures/supp_fig_4b\").with_suffix(\".svg\"),\n",
    "    width=900,\n",
    "    height=300,\n",
    "    scale=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0fc0c3",
   "metadata": {},
   "source": [
    "## [Supp Fig 4c]. RMSE over tumour purity level across cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802c3c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need to show all 19 tumour purities for each tool. Just half of it will be fine\n",
    "# Getting tumour purity levels with intervals of 15% instead of 10%\n",
    "reduced_purity_levels = np.arange(0.05, 1, 0.15).round(3).tolist()\n",
    "\n",
    "methods = [\n",
    "    \"bprism_v2\",\n",
    "    \"music\",\n",
    "    \"cpm\",\n",
    "    \"cbx\",\n",
    "    \"scaden\",\n",
    "    \"epic\",\n",
    "    \"bisque\",\n",
    "    \"dwls\",\n",
    "    \"hspe\",\n",
    "]\n",
    "\n",
    "ctypes_order = [\n",
    "    \"Plasmablasts\",\n",
    "    \"PVL\",\n",
    "    \"CAFs\",\n",
    "    \"Endothelial\",\n",
    "    \"Myeloid\",\n",
    "    \"B-cells\",\n",
    "    \"T-cells\",\n",
    "    \"Normal Epithelial\",\n",
    "    \"Cancer Epithelial\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6d9b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_epithelial(source_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Collapse Normal Epithelial and Cancer Epithelial into Epithelial\n",
    "\n",
    "    Args:\n",
    "        source_df:      predictions or groundtruth DataFrame. Rows as mixtures, columns as cell types\n",
    "    \"\"\"\n",
    "    # Copy provided DataFrame so we don't alter it\n",
    "    df = source_df.copy()\n",
    "\n",
    "    # Collapse Cancer and Normal Epithelial, and then drop these cols\n",
    "    df[\"Epithelial\"] = df[\"Normal Epithelial\"] + df[\"Cancer Epithelial\"]\n",
    "    df.drop([\"Normal Epithelial\", \"Cancer Epithelial\"], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b560e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(\n",
    "    subset_truth_df: pd.DataFrame, subset_res_df: pd.DataFrame, c_types: List\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Iterate over provided cell types and calculate peformance metrics of predictions against groundtruth\n",
    "    The method assumes that provided cell types are consistent across both prediction and groundtruth DataFrame\n",
    "\n",
    "    Args:\n",
    "        - subset_truth_df:     groundtruth DataFrame, purity-level-specific\n",
    "        - subset_res_df:     predictions DataFrame, purity-level-specific\n",
    "        - c_types:             cell types to iterate over\n",
    "    \"\"\"\n",
    "    # Create an empty list to hold peformance metrics of each cell type\n",
    "    metrics_series_l = []\n",
    "\n",
    "    # Iterate over cell types and calcuate RMSE + MAE + Cosine\n",
    "    for c_type in c_types:\n",
    "        # Re-arrange colums both predictions and groundtruth in the same order\n",
    "        ctype_truth_df = subset_truth_df[c_type]\n",
    "        ctype_preds_df = subset_res_df[c_type]\n",
    "\n",
    "        # RMSE\n",
    "        rmse = sqrt(mean_squared_error(ctype_truth_df * 100, ctype_preds_df * 100))\n",
    "\n",
    "        # MAE\n",
    "        mae = abs(ctype_truth_df - ctype_preds_df).median() * 100\n",
    "\n",
    "        # RPE\n",
    "        rpe = (\n",
    "            abs(ctype_truth_df - ctype_preds_df) / ctype_truth_df.replace({0: 0.0001})\n",
    "        ).median()\n",
    "\n",
    "        metrics_series_l.append(pd.Series([rmse, mae, rpe], name=c_type))\n",
    "\n",
    "    # Concatenate metrics across cell types\n",
    "    method_metrics_df = pd.concat(metrics_series_l, axis=1)\n",
    "    method_metrics_df.index = [\"RMSE\", \"MAE\", \"RPE\"]\n",
    "\n",
    "    return method_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadec9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rmse_heatmap(\n",
    "    avg_diff_df: pd.DataFrame,\n",
    "    outfile_name: str,\n",
    "    metric: str,\n",
    "    metric_suffix: str,\n",
    "    colorscale: str,\n",
    "    c_types: List,\n",
    "    plot_w: int,\n",
    "    plot_h: int,\n",
    "    z_range: List = [0, 50],\n",
    "    dticks: int = 10,\n",
    "    auto_open: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Plot heatmap of Mean Absolute Error across tumour purity levels\n",
    "\n",
    "    Args:\n",
    "        - avg_diff_df:        DataFrame holding MAE over tumour purity levels\n",
    "        - outfile_name:       name of output html and png files\n",
    "        - c_types:            cell types in a specific order we'd like to appear on the y-axis\n",
    "        - z_range:            Maximum error (between Scaden, CBX and EPIC) is ~56%\n",
    "                              so we only need to set maximum zaxis to 50%.\n",
    "                              This ensure extreme errors are very red on the scale\n",
    "        - auto_open:          Whether to open html after creation or not\n",
    "\n",
    "    \"\"\"\n",
    "    # Create annotated heatmap object with plotly\n",
    "    fig = ff.create_annotated_heatmap(\n",
    "        z=avg_diff_df.values,\n",
    "        # Annotate each cell in the heatmap with the corresponding labels\n",
    "        annotation_text=avg_diff_df.values.round(1).astype(str),\n",
    "        zmin=z_range[0],\n",
    "        zmax=z_range[1],\n",
    "        x=(avg_diff_df.columns * 100).astype(int).tolist(),  # Rows are purity levels\n",
    "        y=avg_diff_df.index.tolist(),  # Columns are cell types\n",
    "        colorscale=colorscale,\n",
    "        showscale=False,\n",
    "        hoverinfo=\"text\",\n",
    "        text=avg_diff_df.values.round(1),\n",
    "        colorbar=dict(\n",
    "            title=metric,\n",
    "            ticks=\"outside\",\n",
    "            ticksuffix=metric_suffix,\n",
    "            dtick=dticks,\n",
    "            orientation=\"h\",\n",
    "            ticklen=2,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Update axes\n",
    "    fig.update_xaxes(\n",
    "        title=\"Tumour purity levels (%)\",\n",
    "        title_font_size=8,\n",
    "        title_standoff=5,\n",
    "        ticks=\"outside\",\n",
    "        tickfont_size=7,\n",
    "        showticklabels=True,\n",
    "        ticklen=2,\n",
    "        tickwidth=0.5,\n",
    "        tickmode=\"array\",\n",
    "        tickvals=(avg_diff_df.columns * 100).astype(int).tolist(),\n",
    "        linecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "        side=\"bottom\",\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        title=\"Cell Types\",\n",
    "        title_font_size=8,\n",
    "        title_standoff=5,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "        categoryorder=\"array\",\n",
    "        categoryarray=c_types,  # Order cell types by linages\n",
    "        ticks=\"outside\",\n",
    "        showticklabels=True,\n",
    "        ticklen=2,\n",
    "        tickwidth=0.5,\n",
    "        tickfont_size=7,\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig[\"layout\"].update(\n",
    "        margin=dict(t=0, l=0, r=0, b=0),\n",
    "        font_size=6,\n",
    "        plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        font_color=\"black\",\n",
    "    )\n",
    "\n",
    "    # Save offline mode\n",
    "    fig.write_image(\n",
    "        Path(f\"{outfile_name}\").with_suffix(\".svg\"),\n",
    "        width=plot_w,\n",
    "        height=plot_h,\n",
    "        scale=5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bddee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes to attach to Sunburst plot for each metrics\n",
    "metrics_mapping = {\n",
    "    \"MAE\": {\n",
    "        \"colorscale\": \"purples\",\n",
    "        \"zmin\": 0,\n",
    "        \"zmax\": 50,\n",
    "        \"title\": \"Mean Absolute Error\",\n",
    "        \"tick_suffix\": \"\",\n",
    "        \"d_tick\": 10,\n",
    "    },\n",
    "    \"RMSE\": {\n",
    "        \"colorscale\": \"reds\",\n",
    "        \"zmin\": 0,\n",
    "        \"zmax\": 50,\n",
    "        \"title\": \"RMSE\",\n",
    "        \"tick_suffix\": \"\",\n",
    "        \"d_tick\": 10,\n",
    "    },\n",
    "    \"RPE\": {\n",
    "        \"colorscale\": \"oranges\",\n",
    "        \"zmin\": 0,\n",
    "        \"zmax\": 1000,\n",
    "        \"title\": \"RPE\",\n",
    "        \"tick_suffix\": \" %\",\n",
    "        \"d_tick\": 100,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17094b90",
   "metadata": {},
   "source": [
    "#### Plot Cancer and Normal Epithelial separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d954f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rmse_l = []\n",
    "\n",
    "# For each tool, calculate average absolute error across tumour purity levels\n",
    "for method in tqdm(methods):\n",
    "    print(f\"Generating performance metrics for {method}\")\n",
    "    res_df = pd.read_csv(\n",
    "        Path(prefix).joinpath(f\"data/results/{method}.csv\"), sep=\"\\t\", index_col=0\n",
    "    )\n",
    "\n",
    "    # Some values can be ~-0.00001..., replace them by 0\n",
    "    res_df.clip(lower=0, inplace=True)\n",
    "\n",
    "    # Empty list to hold metrics\n",
    "    all_metrics_l = []\n",
    "\n",
    "    for pur_lvl in tqdm(purity_levels):\n",
    "        # Calculate average of the absolute difference for each purity level\n",
    "        subset_truth_df = truth_df[\n",
    "            truth_df[\"Cancer Epithelial\"] == pur_lvl\n",
    "        ].sort_index()\n",
    "        subset_res_df = res_df[res_df.index.isin(subset_truth_df.index)].sort_index()\n",
    "\n",
    "        pur_lvl_metrics_df = calculate_metrics(\n",
    "            subset_truth_df=subset_truth_df,\n",
    "            subset_res_df=subset_res_df,\n",
    "            c_types=c_types,\n",
    "        )\n",
    "        pur_lvl_metrics_df[\"Purity Level\"] = pur_lvl\n",
    "        all_metrics_l.append(pur_lvl_metrics_df)\n",
    "\n",
    "    # Concatenate\n",
    "    all_metrics_df = pd.concat(all_metrics_l, axis=0)\n",
    "    all_metrics_df = all_metrics_df.round(2)\n",
    "\n",
    "    # Plot each metric separately\n",
    "    for metric in [\"RMSE\"]:\n",
    "        # Plot reduced tumour purity levels\n",
    "        reduced_pur_lvl_df = (\n",
    "            all_metrics_df.loc[\n",
    "                (all_metrics_df[\"Purity Level\"].isin(reduced_purity_levels))\n",
    "                & (all_metrics_df.index == \"RMSE\")\n",
    "            ]\n",
    "            .set_index([\"Purity Level\"])\n",
    "            .T\n",
    "        )\n",
    "\n",
    "        plot_metrics_df = all_metrics_df.loc[metric].set_index([\"Purity Level\"]).T\n",
    "\n",
    "        # Plot all tumour purity levels\n",
    "        all_pur_lvl_df = all_metrics_df.loc[\"RMSE\"].set_index([\"Purity Level\"]).T\n",
    "\n",
    "        # Plot heat map\n",
    "        plot_rmse_heatmap(\n",
    "            avg_diff_df=all_pur_lvl_df.round(1),\n",
    "            outfile_name=f\"figures/supp_figures/supp_fig_4c_{metric}_{method}\",\n",
    "            metric=metric,\n",
    "            metric_suffix=metrics_mapping[metric][\"tick_suffix\"],\n",
    "            c_types=ctypes_order,\n",
    "            z_range=[metrics_mapping[metric][\"zmin\"], metrics_mapping[metric][\"zmax\"]],\n",
    "            dticks=metrics_mapping[metric][\"d_tick\"],\n",
    "            colorscale=metrics_mapping[metric][\"colorscale\"],\n",
    "            plot_w=360,\n",
    "            plot_h=125,\n",
    "        )\n",
    "\n",
    "        # Collect RMSE and method\n",
    "        rmse_df = (\n",
    "            all_metrics_df.loc[\"RMSE\"].reset_index().rename(columns={\"index\": \"metric\"})\n",
    "        )\n",
    "        rmse_df[\"method\"] = method\n",
    "        all_rmse_l.append(rmse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save source data\n",
    "all_rmse_df = pd.concat(all_rmse_l, axis=0)\n",
    "all_rmse_df.replace(\n",
    "    {\n",
    "        \"scaden\": \"Scaden\",\n",
    "        \"music\": \"MuSiC\",\n",
    "        \"cbx\": \"CBX\",\n",
    "        \"bisque\": \"Bisque\",\n",
    "        \"dwls\": \"DWLS\",\n",
    "        \"epic\": \"EPIC\",\n",
    "        \"cpm\": \"CPM\",\n",
    "        \"bprism_v2\": \"BayesPrism\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "all_rmse_df[\"Purity Level\"] = all_rmse_df[\"Purity Level\"] * 100\n",
    "all_rmse_df.to_csv(\n",
    "    Path(viz_prefix).joinpath(\"source_data/supp_figure_4c.tsv\"), sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4429df87",
   "metadata": {},
   "source": [
    "#### Collapse Cancer and Normal Epithelial into Epithelial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d487e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_epithelial_ctypes_order = [\n",
    "    \"Plasmablasts\",\n",
    "    \"PVL\",\n",
    "    \"CAFs\",\n",
    "    \"Endothelial\",\n",
    "    \"Myeloid\",\n",
    "    \"B-cells\",\n",
    "    \"T-cells\",\n",
    "    \"Epithelial\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cba054",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rmse_l = []\n",
    "\n",
    "# For each tool, collapse Normal Epithelial and Cancer Epithelial into Epithelial\n",
    "# Then calculate average absolute error across tumour purity levels\n",
    "for method in tqdm(methods):\n",
    "    print(f\"Generating performance metrics for {method}\")\n",
    "    res_df = pd.read_csv(\n",
    "        Path(prefix).joinpath(f\"data/results/{method}.csv\"), sep=\"\\t\", index_col=0\n",
    "    )\n",
    "\n",
    "    # Some values can be ~-0.00001..., replace them by 0\n",
    "    res_df[res_df < 0] = 0\n",
    "\n",
    "    # Collapse Cancer and Epithelial in predictions and groundtruth DataFrames\n",
    "    collapsed_res_df = collapse_epithelial(source_df=res_df)\n",
    "    collapsed_truth_df = collapse_epithelial(source_df=truth_df)\n",
    "\n",
    "    # Empty list to hold metrics\n",
    "    all_metrics_l = []\n",
    "\n",
    "    for pur_lvl in tqdm(purity_levels):\n",
    "        # Calculate average of the absolute difference for each purity level\n",
    "        subset_truth_df = collapsed_truth_df[\n",
    "            truth_df[\"Cancer Epithelial\"] == pur_lvl\n",
    "        ].sort_index()\n",
    "        subset_res_df = collapsed_res_df[\n",
    "            collapsed_res_df.index.isin(subset_truth_df.index)\n",
    "        ].sort_index()\n",
    "\n",
    "        pur_lvl_metrics_df = calculate_metrics(\n",
    "            subset_truth_df=subset_truth_df,\n",
    "            subset_res_df=subset_res_df,\n",
    "            c_types=grouped_epithelial_ctypes_order,\n",
    "        )\n",
    "        pur_lvl_metrics_df[\"Purity Level\"] = pur_lvl\n",
    "        all_metrics_l.append(pur_lvl_metrics_df)\n",
    "\n",
    "    all_metrics_df = pd.concat(all_metrics_l, axis=0)\n",
    "    all_metrics_df = all_metrics_df.round(2)\n",
    "\n",
    "    # Plot each metric separately\n",
    "    for metric in [\"RMSE\"]:\n",
    "        # Plot reduced tumour purity levels\n",
    "        reduced_pur_lvl_df = (\n",
    "            all_metrics_df.loc[\n",
    "                (all_metrics_df[\"Purity Level\"].isin(reduced_purity_levels))\n",
    "                & (all_metrics_df.index == \"RMSE\")\n",
    "            ]\n",
    "            .set_index([\"Purity Level\"])\n",
    "            .T\n",
    "        )\n",
    "\n",
    "        plot_metrics_df = all_metrics_df.loc[metric].set_index([\"Purity Level\"]).T\n",
    "\n",
    "        # Plot all tumour purity levels\n",
    "        all_pur_lvl_df = all_metrics_df.loc[\"RMSE\"].set_index([\"Purity Level\"]).T\n",
    "\n",
    "        # Plot heatmap\n",
    "        plot_rmse_heatmap(\n",
    "            avg_diff_df=all_pur_lvl_df.round(1),\n",
    "            outfile_name=f\"figures/supp_figures/supp_fig_11a_{metric}_{method}\",\n",
    "            metric=metric,\n",
    "            metric_suffix=metrics_mapping[metric][\"tick_suffix\"],\n",
    "            c_types=grouped_epithelial_ctypes_order,\n",
    "            z_range=[metrics_mapping[metric][\"zmin\"], metrics_mapping[metric][\"zmax\"]],\n",
    "            dticks=metrics_mapping[metric][\"d_tick\"],\n",
    "            colorscale=metrics_mapping[metric][\"colorscale\"],\n",
    "            plot_w=310,\n",
    "            plot_h=125,\n",
    "        )\n",
    "\n",
    "        # Collect RMSE and method\n",
    "        rmse_df = (\n",
    "            all_metrics_df.loc[\"RMSE\"].reset_index().rename(columns={\"index\": \"metric\"})\n",
    "        )\n",
    "        rmse_df[\"method\"] = method\n",
    "        all_rmse_l.append(rmse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34185689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save source data\n",
    "all_rmse_df = pd.concat(all_rmse_l, axis=0)\n",
    "all_rmse_df.replace(\n",
    "    {\n",
    "        \"scaden\": \"Scaden\",\n",
    "        \"music\": \"MuSiC\",\n",
    "        \"cbx\": \"CBX\",\n",
    "        \"bisque\": \"Bisque\",\n",
    "        \"dwls\": \"DWLS\",\n",
    "        \"epic\": \"EPIC\",\n",
    "        \"cpm\": \"CPM\",\n",
    "        \"bprism_v2\": \"BayesPrism\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "all_rmse_df[\"Purity Level\"] = all_rmse_df[\"Purity Level\"] * 100\n",
    "all_rmse_df.to_csv(\n",
    "    Path(viz_prefix).joinpath(\"source_data/supp_figure_11a.tsv\"), sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a78c5df",
   "metadata": {},
   "source": [
    "#### For mixtures with only Cancer Epithelial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_normal_ctypes_order = [\n",
    "    \"Plasmablasts\",\n",
    "    \"PVL\",\n",
    "    \"CAFs\",\n",
    "    \"Endothelial\",\n",
    "    \"Myeloid\",\n",
    "    \"B-cells\",\n",
    "    \"T-cells\",\n",
    "    \"Cancer Epithelial\",\n",
    "]\n",
    "\n",
    "# For this plot, we'll also need predictions and groundtruth from without-Normal experiment\n",
    "no_normal_prefix = \"???/deconvolution_benchmarking/01_purity_levels_experiment\"\n",
    "\n",
    "no_normal_truth_df = pd.read_csv(\n",
    "    Path(no_normal_prefix).joinpath(\"data/results/truth.csv\"), sep=\"\\t\", index_col=0\n",
    ")\n",
    "no_normal_truth_df = no_normal_truth_df[\n",
    "    [\n",
    "        \"B-cells\",\n",
    "        \"CAFs\",\n",
    "        \"Cancer Epithelial\",\n",
    "        \"Endothelial\",\n",
    "        \"Myeloid\",\n",
    "        \"PVL\",\n",
    "        \"Plasmablasts\",\n",
    "        \"T-cells\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dce8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rmse_l = []\n",
    "\n",
    "# For each tool, collapse Normal Epithelial and Cancer Epithelial into Epithelial\n",
    "# Then calculate average absolute error across tumour purity levels\n",
    "for method in tqdm(methods):\n",
    "    print(f\"Generating performance metrics for {method}\")\n",
    "    res_df = pd.read_csv(\n",
    "        Path(no_normal_prefix).joinpath(f\"data/results/{method}.csv\"),\n",
    "        sep=\"\\t\",\n",
    "        index_col=0,\n",
    "    )\n",
    "\n",
    "    # Some values can be ~-0.00001..., replace them by 0\n",
    "    res_df.clip(lower=0)\n",
    "\n",
    "    # Empty list to hold metrics\n",
    "    all_metrics_l = []\n",
    "\n",
    "    for pur_lvl in tqdm(purity_levels):\n",
    "        # Calculate average of the absolute difference for each purity level\n",
    "        subset_truth_df = no_normal_truth_df[\n",
    "            no_normal_truth_df[\"Cancer Epithelial\"] == pur_lvl\n",
    "        ].sort_index()\n",
    "        subset_res_df = res_df[res_df.index.isin(subset_truth_df.index)].sort_index()\n",
    "\n",
    "        pur_lvl_metrics_df = calculate_metrics(\n",
    "            subset_truth_df=subset_truth_df,\n",
    "            subset_res_df=subset_res_df,\n",
    "            c_types=no_normal_ctypes_order,\n",
    "        )\n",
    "        pur_lvl_metrics_df[\"Purity Level\"] = pur_lvl\n",
    "        all_metrics_l.append(pur_lvl_metrics_df)\n",
    "\n",
    "    all_metrics_df = pd.concat(all_metrics_l, axis=0)\n",
    "    all_metrics_df = all_metrics_df.round(2)\n",
    "\n",
    "    # Plot each metric separately\n",
    "    for metric in [\"RMSE\"]:\n",
    "        # Plot reduced tumour purity levels\n",
    "        reduced_pur_lvl_df = (\n",
    "            all_metrics_df.loc[\n",
    "                (all_metrics_df[\"Purity Level\"].isin(reduced_purity_levels))\n",
    "                & (all_metrics_df.index == \"RMSE\")\n",
    "            ]\n",
    "            .set_index([\"Purity Level\"])\n",
    "            .T\n",
    "        )\n",
    "\n",
    "        plot_metrics_df = all_metrics_df.loc[metric].set_index([\"Purity Level\"]).T\n",
    "\n",
    "        # Plot all tumour purity levels\n",
    "        all_pur_lvl_df = all_metrics_df.loc[\"RMSE\"].set_index([\"Purity Level\"]).T\n",
    "\n",
    "        # Plot heatmap\n",
    "        plot_rmse_heatmap(\n",
    "            avg_diff_df=all_pur_lvl_df,\n",
    "            outfile_name=f\"figures/supp_figures/supp_fig_11b_{metric}_all_{method}\",\n",
    "            metric=metric,\n",
    "            metric_suffix=metrics_mapping[metric][\"tick_suffix\"],\n",
    "            c_types=no_normal_ctypes_order,\n",
    "            z_range=[metrics_mapping[metric][\"zmin\"], metrics_mapping[metric][\"zmax\"]],\n",
    "            dticks=metrics_mapping[metric][\"d_tick\"],\n",
    "            colorscale=metrics_mapping[metric][\"colorscale\"],\n",
    "            plot_w=310,\n",
    "            plot_h=125,\n",
    "        )\n",
    "\n",
    "        # Collect RMSE and method\n",
    "        rmse_df = (\n",
    "            all_metrics_df.loc[\"RMSE\"].reset_index().rename(columns={\"index\": \"metric\"})\n",
    "        )\n",
    "        rmse_df[\"method\"] = method\n",
    "        all_rmse_l.append(rmse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465224a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save source data\n",
    "all_rmse_df = pd.concat(all_rmse_l, axis=0)\n",
    "all_rmse_df.replace(\n",
    "    {\n",
    "        \"scaden\": \"Scaden\",\n",
    "        \"music\": \"MuSiC\",\n",
    "        \"cbx\": \"CBX\",\n",
    "        \"bisque\": \"Bisque\",\n",
    "        \"dwls\": \"DWLS\",\n",
    "        \"epic\": \"EPIC\",\n",
    "        \"cpm\": \"CPM\",\n",
    "        \"bprism_v2\": \"BayesPrism\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "all_rmse_df.to_csv(\n",
    "    Path(viz_prefix).joinpath(\"source_data/supp_figure_11b.tsv\"), sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "754fca49",
   "metadata": {},
   "source": [
    "## [Extended Data Fig 1a]. Bray Curtis dissimilarity across tumour purity levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42004189",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\n",
    "    \"bprism_v2\",\n",
    "    \"scaden\",\n",
    "    \"music\",\n",
    "    \"cpm\",\n",
    "    \"cbx\",\n",
    "    \"hspe\",\n",
    "    \"epic\",\n",
    "    \"bisque\",\n",
    "    \"dwls\",\n",
    "]\n",
    "\n",
    "# We don't need to show all 19 tumour purities for each tool. Just half of it will be fine\n",
    "# Getting tumour purity levels with intervals of 15% instead of 10%\n",
    "reduced_purity_levels = np.arange(0.05, 1, 0.15).round(3).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e907aa8c",
   "metadata": {},
   "source": [
    "#### Calculate Bray-Curtis dissmilarity index across tumour purity levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac21d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "bray_curtis_l = []\n",
    "\n",
    "for method in tqdm(methods):\n",
    "    res_df = pd.read_csv(\n",
    "        Path(prefix).joinpath(f\"data/results/{method}.csv\"), sep=\"\\t\", index_col=0\n",
    "    )\n",
    "    res_df = res_df[c_types]\n",
    "\n",
    "    # Clip tiny negative numbers to 0\n",
    "    res_df.clip(lower=0, inplace=True)\n",
    "\n",
    "    # Check if indexes match\n",
    "    assert (res_df.sort_index().index == truth_df.sort_index().index).all()\n",
    "    assert (res_df.sort_index().columns == truth_df.sort_index().columns).all()\n",
    "\n",
    "    # Iterate over res_df and calculate Bray-Curtis index\n",
    "    for sample_id in res_df.index:\n",
    "        bray_curtis_dissi = braycurtis(res_df.loc[sample_id], truth_df.loc[sample_id])\n",
    "        bray_curtis_l.append(\n",
    "            (\n",
    "                sample_id,\n",
    "                bray_curtis_dissi,\n",
    "                truth_df.loc[sample_id, \"Cancer Epithelial\"],\n",
    "                method,\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Concatenate all rmse dataframes\n",
    "bray_curtis_df = pd.DataFrame(\n",
    "    bray_curtis_l, columns=[\"Mixture ID\", \"Bray Curtis Dissi\", \"Purity Level\", \"Method\"]\n",
    ").set_index([\"Mixture ID\"])\n",
    "\n",
    "# Rename method names\n",
    "bray_curtis_df.replace(\n",
    "    {\n",
    "        \"scaden\": \"Scaden\",\n",
    "        \"music\": \"MuSiC\",\n",
    "        \"cbx\": \"CBX\",\n",
    "        \"bisque\": \"Bisque\",\n",
    "        \"dwls\": \"DWLS\",\n",
    "        \"epic\": \"EPIC\",\n",
    "        \"cpm\": \"CPM\",\n",
    "        \"bprism_v2\": \"BayesPrism\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Convert purity level to % scale and categorical\n",
    "bray_curtis_df[\"Purity Level\"] = (bray_curtis_df[\"Purity Level\"] * 100).astype(int)\n",
    "bray_curtis_df[\"Purity Level\"] = bray_curtis_df[\"Purity Level\"].astype(\"category\")\n",
    "bray_curtis_df[\"Purity Level\"].cat.reorder_categories(\n",
    "    [int(i * 100) for i in purity_levels], inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f39be",
   "metadata": {},
   "source": [
    "#### Heatmaps of median Bray Curtis dissimilarity across tumour purity levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a787347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get median of bray-curtis at each tumour purity level\n",
    "median_bray_curtis_df = bray_curtis_df.groupby([\"Method\", \"Purity Level\"]).agg(\n",
    "    [\"median\"]\n",
    ")\n",
    "\n",
    "# 1st level of multi-index column is redundant, drop it and reset index\n",
    "median_bray_curtis_df = median_bray_curtis_df.droplevel(0, axis=1).reset_index()\n",
    "\n",
    "# Pivot wider so we have purity levels as columns (x-axis) and methods as rows (y-axis)\n",
    "pivot_median_bray_curtis_df = median_bray_curtis_df.pivot(\n",
    "    index=\"Method\", columns=\"Purity Level\", values=\"median\"\n",
    ")\n",
    "\n",
    "# y-axis order is reverse\n",
    "pivot_median_bray_curtis_df = pivot_median_bray_curtis_df.loc[methods_order[::-1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7cef92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save source data\n",
    "median_bray_curtis_df.to_csv(\n",
    "    Path(viz_prefix).joinpath(\"source_data/supp_figure_4a.tsv\"), sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d05bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ff.create_annotated_heatmap(\n",
    "    z=pivot_median_bray_curtis_df.values,\n",
    "    annotation_text=pivot_median_bray_curtis_df.values.round(2),\n",
    "    zmin=0,\n",
    "    zmax=1,\n",
    "    x=pivot_median_bray_curtis_df.columns.tolist(),\n",
    "    y=pivot_median_bray_curtis_df.index.tolist(),\n",
    "    colorscale=\"teal\",\n",
    "    showscale=True,\n",
    "    hoverinfo=\"text\",\n",
    "    text=pivot_median_bray_curtis_df.round(2).values,\n",
    "    colorbar=dict(\n",
    "        title=\"Bray-Curis<br>Dissimilarity\",\n",
    "        titlefont_size=14,\n",
    "        ticks=\"outside\",\n",
    "        ticksuffix=\"\",\n",
    "        tickfont_size=12,\n",
    "        dtick=0.1,\n",
    "        orientation=\"v\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig[\"layout\"].update(\n",
    "    margin=dict(t=0, l=0, r=0, b=0),\n",
    "    plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "    font=dict(size=12, color=\"black\"),\n",
    "    xaxis=dict(\n",
    "        title=\"Tumour purity levels (%)\",\n",
    "        titlefont_size=14,\n",
    "        title_standoff=5,\n",
    "        ticks=\"outside\",\n",
    "        showticklabels=True,\n",
    "        tickmode=\"array\",\n",
    "        tickwidth=0.75,\n",
    "        ticklen=3,\n",
    "        tickvals=[int(i * 100) for i in purity_levels],\n",
    "        tickfont_size=12,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=0.75,\n",
    "        side=\"bottom\",\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Methods\",\n",
    "        titlefont_size=14,\n",
    "        title_standoff=1,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=0.75,\n",
    "        ticks=\"outside\",\n",
    "        tickwidth=0.75,\n",
    "        ticklen=3,\n",
    "        tickfont_size=12,\n",
    "        categoryorder=\"array\",\n",
    "        categoryarray=methods_order,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.write_image(\n",
    "    Path(\"figures/supp_figures/supp_fig_4a\").with_suffix(\".svg\"),\n",
    "    width=900,\n",
    "    height=300,\n",
    "    scale=5,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e21e3183055704cdc6beb302a7eaad42e1c0671a451dc4bde87185c59632390"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
