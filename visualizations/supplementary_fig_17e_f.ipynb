{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b7cc55-3b46-46c2-bdd6-6e34ffdc37f1",
   "metadata": {},
   "source": [
    "# Visualize models performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09a63a1-8e52-493c-b685-5a419fa97191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as adata\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly as plotly\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity as skl_cosine\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import braycurtis, cdist\n",
    "from math import sqrt\n",
    "\n",
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52cef7f-433e-4db0-8f5f-942d374522df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set prefixes\n",
    "prefix = \"???/deconvolution_benchmarking/06_batch_effect_validation/pal_et_al\"\n",
    "\n",
    "# Prefix to visualizations folder\n",
    "viz_prefix = \"???/deconvolution_benchmarking/visualizations\"\n",
    "\n",
    "# Major cell types\n",
    "c_types = [\n",
    "    \"Cancer Epithelial\",\n",
    "    \"Normal Epithelial\",\n",
    "    \"T-cells\",\n",
    "    \"B-cells\",\n",
    "    \"Myeloid\",\n",
    "    \"CAFs\",\n",
    "    \"Endothelial\",\n",
    "    \"PVL\",\n",
    "    \"Plasmablasts\",\n",
    "]\n",
    "c_types_mapping = {\n",
    "    \"B_cells\": \"B-cells\",\n",
    "    \"Cancer_Epithelial\": \"Cancer Epithelial\",\n",
    "    \"Normal_Epithelial\": \"Normal Epithelial\",\n",
    "    \"T_cells\": \"T-cells\",\n",
    "}\n",
    "# List tumour purity levels\n",
    "purity_levels = np.arange(0.05, 1, 0.05).round(3).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77080247",
   "metadata": {},
   "source": [
    "### Load groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f562086a-5077-43a2-989e-422ee88109af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load truth.csv\n",
    "truth_df = pd.read_csv(\n",
    "    Path(prefix).joinpath(\"data/results/truth.tsv\"), sep=\"\\t\", index_col=0\n",
    ")\n",
    "truth_df.rename(columns=c_types_mapping, inplace=True)\n",
    "truth_df = truth_df[c_types]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859167e",
   "metadata": {},
   "source": [
    "## [Fig]. RMSE over tumour purity level across cell type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee41c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need to show all 19 tumour purities for each tool. Just half of it will be fine\n",
    "# Getting tumour purity levels with intervals of 15% instead of 10%\n",
    "reduced_purity_levels = np.arange(0.05, 1, 0.15).round(3).tolist()\n",
    "\n",
    "methods = [\"bprism_v2\", \"bprism_v2_initial_gibbs_sampling\"]\n",
    "\n",
    "ctypes_order = [\n",
    "    \"Plasmablasts\",\n",
    "    \"PVL\",\n",
    "    \"CAFs\",\n",
    "    \"Endothelial\",\n",
    "    \"Myeloid\",\n",
    "    \"B-cells\",\n",
    "    \"T-cells\",\n",
    "    \"Normal Epithelial\",\n",
    "    \"Cancer Epithelial\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf57f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(\n",
    "    subset_truth_df: pd.DataFrame, subset_res_df: pd.DataFrame, c_types: List\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Iterate over provided cell types and calculate peformance metrics of predictions against groundtruth\n",
    "    The method assumes that provided cell types are consistent across both prediction and groundtruth DataFrame\n",
    "\n",
    "    Args:\n",
    "        - subset_truth_df:     groundtruth DataFrame, purity-level-specific\n",
    "        - subset_res_df:     predictions DataFrame, purity-level-specific\n",
    "        - c_types:             cell types to iterate over\n",
    "    \"\"\"\n",
    "    # Create an empty list to hold peformance metrics of each cell type\n",
    "    metrics_series_l = []\n",
    "\n",
    "    # Iterate over cell types and calcuate RMSE + MAE + Cosine\n",
    "    for c_type in c_types:\n",
    "        # Re-arrange colums both predictions and groundtruth in the same order\n",
    "        ctype_truth_df = subset_truth_df[c_type]\n",
    "        ctype_preds_df = subset_res_df[c_type]\n",
    "\n",
    "        # RMSE\n",
    "        rmse = sqrt(mean_squared_error(ctype_truth_df * 100, ctype_preds_df * 100))\n",
    "\n",
    "        # MAE\n",
    "        mae = abs(ctype_truth_df - ctype_preds_df).median() * 100\n",
    "\n",
    "        # RPE\n",
    "        rpe = (\n",
    "            abs(ctype_truth_df - ctype_preds_df) / ctype_truth_df.replace({0: 0.0001})\n",
    "        ).median()\n",
    "\n",
    "        metrics_series_l.append(pd.Series([rmse, mae, rpe], name=c_type))\n",
    "\n",
    "    # Concatenate metrics across cell types\n",
    "    method_metrics_df = pd.concat(metrics_series_l, axis=1)\n",
    "    method_metrics_df.index = [\"RMSE\", \"MAE\", \"RPE\"]\n",
    "\n",
    "    return method_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5011cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rmse_heatmap(\n",
    "    avg_diff_df: pd.DataFrame,\n",
    "    outfile_name: str,\n",
    "    metric: str,\n",
    "    metric_suffix: str,\n",
    "    colorscale: str,\n",
    "    c_types: List,\n",
    "    plot_w: int,\n",
    "    plot_h: int,\n",
    "    z_range: List = [0, 50],\n",
    "    dticks: int = 10,\n",
    "    auto_open: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Plot heatmap of Mean Absolute Error across tumour purity levels\n",
    "\n",
    "    Args:\n",
    "        - avg_diff_df:        DataFrame holding MAE over tumour purity levels\n",
    "        - outfile_name:       name of output html and png files\n",
    "        - c_types:            cell types in a specific order we'd like to appear on the y-axis\n",
    "        - z_range:            Maximum error (between Scaden, CBX and EPIC) is ~56%\n",
    "                              so we only need to set maximum zaxis to 50%.\n",
    "                              This ensure extreme errors are very red on the scale\n",
    "        - auto_open:          Whether to open html after creation or not\n",
    "\n",
    "    \"\"\"\n",
    "    # Create annotated heatmap object with plotly\n",
    "    fig = ff.create_annotated_heatmap(\n",
    "        z=avg_diff_df.values,\n",
    "        # Annotate each cell in the heatmap with the corresponding labels\n",
    "        annotation_text=avg_diff_df.values.round(2).astype(str),\n",
    "        zmin=z_range[0],\n",
    "        zmax=z_range[1],\n",
    "        x=(avg_diff_df.columns * 100).astype(int).tolist(),  # Rows are purity levels\n",
    "        y=avg_diff_df.index.tolist(),  # Columns are cell types\n",
    "        colorscale=colorscale,\n",
    "        showscale=False,\n",
    "        hoverinfo=\"text\",\n",
    "        text=avg_diff_df.values.round(4),\n",
    "        colorbar=dict(\n",
    "            title=metric,\n",
    "            ticks=\"outside\",\n",
    "            ticksuffix=metric_suffix,\n",
    "            dtick=dticks,\n",
    "            orientation=\"h\",\n",
    "            ticklen=2,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Update axes\n",
    "    fig.update_xaxes(\n",
    "        title=\"Tumour purity levels (%)\",\n",
    "        title_font_size=8,\n",
    "        title_standoff=5,\n",
    "        ticks=\"outside\",\n",
    "        tickfont_size=7,\n",
    "        showticklabels=True,\n",
    "        ticklen=2,\n",
    "        tickwidth=0.5,\n",
    "        tickmode=\"array\",\n",
    "        tickvals=(avg_diff_df.columns * 100).astype(int).tolist(),\n",
    "        linecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "        side=\"bottom\",\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        title=\"Cell Types\",\n",
    "        title_font_size=8,\n",
    "        title_standoff=5,\n",
    "        linecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "        categoryorder=\"array\",\n",
    "        categoryarray=c_types,  # Order cell types by linages\n",
    "        ticks=\"outside\",\n",
    "        showticklabels=True,\n",
    "        ticklen=2,\n",
    "        tickwidth=0.5,\n",
    "        tickfont_size=7,\n",
    "    )\n",
    "\n",
    "    # Update layout\n",
    "    fig[\"layout\"].update(\n",
    "        margin=dict(t=0, l=0, r=0, b=0),\n",
    "        font_size=6,\n",
    "        plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        font_color=\"black\",\n",
    "    )\n",
    "\n",
    "    # Save offline mode\n",
    "    fig.write_image(\n",
    "        Path(f\"{outfile_name}\").with_suffix(\".svg\"),\n",
    "        width=plot_w,\n",
    "        height=plot_h,\n",
    "        scale=5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a869bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes to attach to Sunburst plot for each metrics\n",
    "metrics_mapping = {\n",
    "    \"MAE\": {\n",
    "        \"colorscale\": \"purples\",\n",
    "        \"zmin\": 0,\n",
    "        \"zmax\": 50,\n",
    "        \"title\": \"Mean Absolute Error\",\n",
    "        \"tick_suffix\": \"\",\n",
    "        \"d_tick\": 10,\n",
    "    },\n",
    "    \"RMSE\": {\n",
    "        \"colorscale\": \"reds\",\n",
    "        \"zmin\": 0,\n",
    "        \"zmax\": 50,\n",
    "        \"title\": \"RMSE\",\n",
    "        \"tick_suffix\": \"\",\n",
    "        \"d_tick\": 10,\n",
    "    },\n",
    "    \"RPE\": {\n",
    "        \"colorscale\": \"oranges\",\n",
    "        \"zmin\": 0,\n",
    "        \"zmax\": 1000,\n",
    "        \"title\": \"RPE\",\n",
    "        \"tick_suffix\": \" %\",\n",
    "        \"d_tick\": 100,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d2555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rmse_l = []\n",
    "\n",
    "# For each tool, calculate average absolute error across tumour purity levels\n",
    "for method in tqdm(methods):\n",
    "    print(f\"Generating performance metrics for {method}\")\n",
    "    res_df = pd.read_csv(\n",
    "        Path(prefix).joinpath(f\"data/results/{method}.tsv\"), sep=\"\\t\", index_col=0\n",
    "    )\n",
    "    res_df.rename(columns=c_types_mapping, inplace=True)\n",
    "\n",
    "    # Some values can be ~-0.00001..., replace them by 0\n",
    "    res_df.clip(lower=0, inplace=True)\n",
    "\n",
    "    # Empty list to hold metrics\n",
    "    all_metrics_l = []\n",
    "\n",
    "    for pur_lvl in tqdm(purity_levels):\n",
    "        # Calculate average of the absolute difference for each purity level\n",
    "        subset_truth_df = truth_df[\n",
    "            truth_df[\"Cancer Epithelial\"] == pur_lvl\n",
    "        ].sort_index()\n",
    "        subset_res_df = res_df[res_df.index.isin(subset_truth_df.index)].sort_index()\n",
    "\n",
    "        pur_lvl_metrics_df = calculate_metrics(\n",
    "            subset_truth_df=subset_truth_df,\n",
    "            subset_res_df=subset_res_df,\n",
    "            c_types=ctypes_order,\n",
    "        )\n",
    "        pur_lvl_metrics_df[\"Purity Level\"] = pur_lvl\n",
    "        all_metrics_l.append(pur_lvl_metrics_df)\n",
    "\n",
    "    # Concatenate\n",
    "    all_metrics_df = pd.concat(all_metrics_l, axis=0)\n",
    "    all_metrics_df = all_metrics_df.round(2)\n",
    "\n",
    "    # Plot each metric separately\n",
    "    for metric in [\"RMSE\"]:\n",
    "        # Plot reduced tumour purity levels\n",
    "        reduced_pur_lvl_df = (\n",
    "            all_metrics_df.loc[\n",
    "                (all_metrics_df[\"Purity Level\"].isin(reduced_purity_levels))\n",
    "                & (all_metrics_df.index == \"RMSE\")\n",
    "            ]\n",
    "            .set_index([\"Purity Level\"])\n",
    "            .T\n",
    "        )\n",
    "\n",
    "        if not Path(prefix).joinpath(\"figures/fig/rmse/cancer_normal\").exists():\n",
    "            Path(prefix).joinpath(\"figures/fig/rmse/cancer_normal\").mkdirs(\n",
    "                exist_ok=True, parents=True\n",
    "            )\n",
    "\n",
    "        plot_metrics_df = all_metrics_df.loc[metric].set_index([\"Purity Level\"]).T\n",
    "        plot_rmse_heatmap(\n",
    "            avg_diff_df=reduced_pur_lvl_df,\n",
    "            outfile_name=f\"figures/supp_figures/supp_fig_17_pal_et_al_{metric}_{method}\",\n",
    "            metric=metric,\n",
    "            metric_suffix=metrics_mapping[metric][\"tick_suffix\"],\n",
    "            c_types=ctypes_order,\n",
    "            z_range=[metrics_mapping[metric][\"zmin\"], metrics_mapping[metric][\"zmax\"]],\n",
    "            dticks=metrics_mapping[metric][\"d_tick\"],\n",
    "            colorscale=metrics_mapping[metric][\"colorscale\"],\n",
    "            plot_w=250,\n",
    "            plot_h=125,\n",
    "        )\n",
    "\n",
    "        # Collect RMSE and method\n",
    "        rmse_df = (\n",
    "            all_metrics_df.loc[\"RMSE\"].reset_index().rename(columns={\"index\": \"metric\"})\n",
    "        )\n",
    "        rmse_df[\"method\"] = method\n",
    "        all_rmse_l.append(rmse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9362d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save source data\n",
    "all_rmse_df = pd.concat(all_rmse_l, axis=0)\n",
    "all_rmse_df[\"method\"].replace(\n",
    "    {\n",
    "        \"bprism_v2\": \"BayesPrism-after Gibbs sampling\",\n",
    "        \"bprism_v2_initial_gibbs_sampling\": \"BayesPrism-before Gibbs sampling\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "all_rmse_df.to_csv(\n",
    "    Path(viz_prefix).joinpath(\"source_data/supp_figure_17e_f.tsv\"), sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b7256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e21e3183055704cdc6beb302a7eaad42e1c0671a451dc4bde87185c59632390"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
